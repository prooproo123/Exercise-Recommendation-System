
\chapter{Općenito o ExRec-u}
U radu \citep{exrec} prepoznata je neefikanost prevladavajućih sustava otvorenih online tečajeva (MOOC) zbog dodjeljivanja istih vježbi svim studentima. S druge strane, personalizirani sustavi preporuka zadataka mogu poboljšati efikasnost učenja prilagođavajući se razini znanja svakog studenta (učenika, korisnika). Takvo razmišljanje uvelike odgovara našem problemu. U originalnom radu krenulo se od temeljne pretpostavke da je ubrzavanje učenja temeljni cilj svakog personaliziranog sustava preporuke zadataka u obrazovanju. Rad predlaže personalizirani sustav preporuke zadataka za online učenje poboljšanjem performansi postojećih \textit{knowledge tracing} modela. Napominje se da postojeći modeli praćenja znanja ne koriste informacije o pripadnosti zadataka konceptima. Konkretno, ovim radom modificira se \textit{dynamic key-value memory network knowledge tracing (DKVMN)} model tako da se memorijska struktura temelji na listi koncepata određenog tečaja eksplicitno bilježeći vezu zadataka i koncepata tijekom procesa praćenja znanja. Model je korišten za izgradnju simulatora studenata korištenog za treniranje strategije preporuke zadataka pomoću podržanog učenja. \newline
\newline
\section{Povezani radovi} %višak? %citate dodati na početna poglavlja ukupnog doca
ExRec je sustav sastavljen od praćenja znanja i preporuke zadataka. Nadogradnja je već poznatih i istraženih sustava. Praćenje znanja inače često koristi prije istražen i implementiran Bayesovski model praćenja znanja (BKT). Modelira studentovo znanje koncepta kao binarnu varijablu i, koristeći skriveni Markovljev model, ažurira vjerojatnost njegova ovladavanja konceptom uzimajući u obzir rezultate rješavanja zadataka. Taj je model na razini koncepata i zanemaruje odnose između različitih koncepata.\newline
Drugi pristup može se pronaći u također već istraženom modelu dubokog praćenja znanja (DKT) s povratnom neuronskom mrežom. Modelira znanje studenta kao latentnu varijablu. DKT je korišten i za sustav preporuke.\newline
\newline
Preporuke zadataka većinom koriste heurističke algoritme. Studentu se preporučuje zadatak ako je vjerojatnost da će ga točno riješiti oko 50\%. Optimalnost takvog algoritma je upitna.\newline
Koristi se i pristup određivanja ZPD-a (zona proksimalnog razvoja, engl. \textit{Zone of Proximal Development}) na temelju trenutnog znanja učenika, a zatim se odabire najkorisniji zadatak pomoću algoritma više naoružanih razbojnika (engl.\textit{ multi-armed bandits algorithm}).\newline
SPARFA framework se koristi za procjenu znanja svakog učenika iz njihovih prethodnih rezultata. Zatim koristi te profile znanja kao kontekst i primjenjuje kontekstualni algoritam naoružanih razbojnika za preporuku zadataka kako bi se maksimizirao učenikov neposredan uspjeh, odnosno njegov uspjeh na sljedećem zadatku. Problem ovog algoritma je što uzima u obzir samo sljedeći korak (kratkoročna nagrada) stoga njegova izvedba ne mora biti optimalna.\newline
\newline
\section{Pozadina sustava i dataset}
Originalni ExRec sustav kao dataset uzima uzorke studentskih interakcija iz IPS-a (engl. \textit{Intelligent practice System}).
IPS je kineski online sustav za samostalno učenje. U IPS-u svaki tečaj ima na desetke gradiva, a student sam bira željeno gradivo. Svako gradivo ima 7 stadija iz kojeg je moguće izaći u bilo kojem trenutku (ili promijeniti gradivo):
\begin{enumerate}
\item vježbe za zagrijavanje prije nastave
\item vježbe na nastavi prije predavanja
\item video predavanja
\item vježbe na nastavi nakon predavanja
\item domaća zadaća
\item vježbe pregleda gradiva
\item vježbe pregleda raznih gradiva.
\end{enumerate}
Svaka vježba (zadatak) ima tri hijerarhijske oznake koncepata (\textit{tagove}) koje su im pridijelili eksperti. U stadijima od 1 do 5 uključeni su sadržaji jednog koncepta, dok 6 i 7 sadrže vježbe drugih koncepata zbog procjene naučenosti. Sustav sprema trajanje studentovog učenja u svakom stadiju, vježbe koje polaže i točnost rezultata.\newline
\newline
Za potrebe našeg projekta napravljena je skripta koja pretvara Assistments dataset (skillbuilder.csv) u format potreban za generiranje preporuka.\newline
Za svakog studenta rezervirana su 4 retka:
\begin{itemize}
\item[-]prvi redak je broj odgovorenih pitanja/vježbi,
\item[-]drugi redak je lista identifikacijskih brojeva pitanja,
\item[-]treći redak redoslijed točnosti odgovora na pitanja, a
\item[-]četvrti redak sadrži koncept prve razine svakog pitanja.
\end{itemize}
\section{DKVMN - model za praćenje znanja}
DKVMN se generalno sastoji od statičke matrice (ključa) koji sprema latentne koncepte i dinamičke matrice (vrijednosti) koja sadrži razine savladnosti određenog koncepta. Model računa korelaciju vježbe i latentnog koncepta u ključu. Korelaciju koristi za čitanje studentovih razina savladanost koncepta i predviđa točnost ishoda rješavanja zadatka.\newline
\subsection{Concept Aware struktura}
DKVMN ExRec praćenja znanja poboljšan u aspektima memorijske strukture, težina koncepata znanja te procesa pisanja i čitanja. Osnovni DKVMN dizajn je modificiran tako da memorijska struktura ovisi o listi koncepata nekog tečaja. Na slici
~\ref{fig:exrec1} prikazana je struktura modela. $M^k_t$ je matrica ugrađenih koncepata veličine $M X N$ gdje je $N$ broj memorijskih lokacija, a $M$ je veličina vektora na svakoj lokaciji. $N$ se postavlja na broj koncepata znanja određenog tečaja. Pošto se u radu koristio 1 koncept prve razine, 7 koncepata druge razine te 15 treće razine, N=23. Nakon toga, na svakoj lokaciji koncepta znanja se sprema studentovo znanje.


	\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.8]{exrec1.png}
	\caption{}
	\label{fig:exrec1}
\end{figure}


\subsection{Knowledge Concept Weight}

Pošto je studentovo stanje koncepta znanja spremljeno u memoriji, kada se pojavi novi zadatak dohvaćaju se i osvježavaju samo memorijske lokacije povezane s tom vježbom. Za svaki koncept znanja se računa težina, težine se koriste kako bi se izračunala težinska suma trenutnih stanja koncepata znanja korisnika kako bi se predvidio njegov rezultat na vježbi. Također će se koristiti kako bi se osvježio korisnikovo stanje znanja nakon primitka rezultata od zadatka.

Prvo se dohvaćaju ugrađene vrijednosti od danog zadatka. Kao što se vidi na slici ~\ref{fig:exrec1}, kada zadatak $q_t$ dođe u trenutku $t$ prvo se transformira u ugrađeni vektor $m_t$ kroz ugrađujuću matricu $A$. Tada se KCW računa pomoću algoritma 1 vidljivog na slici ~\ref{fig:exrec2}. Ukratko, DKVMN računa težinske veze između zadataka i skrivenih koncepata znanja, dok se u ovom kodu računaju samo veze između zadataka i poznatih povezanih koncepata, nepovezani koncepti se postavljaju na 0.


	\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.8]{exrec2.png}
	\caption{}
	\label{fig:exrec2}
\end{figure}



\subsection{Proces čitanja}


Nakon što se izračunati KCW iskoristi za izračun težinske sume stanja koncepata znanja određenog korisnika, $r_t=\sum_{i=1}^{N}w_iM^v_t$, na sumu $r_t$ nadodaju se značajke težine zadatka i "stage feature" $d_t$, $g_t$. Rezultat se šalje u potpuno povezani sloj sa aktivacijskom funkcijom Tanh kako bi se dobio "summary" vektor koji sadrži sve informacije o studentovom znanju povezanom s $q_t$ i značajkama zadatka.
\begin{equation}
	f_t=Tanh(W_0^T[r_t,d_t,g_t,m_t])
\end{equation}

Na kraju, $f_t$ prolazi kroz potpuno povezani sloj koji daje kao izlaz vjerojatnosti da bi student odradio zadatake $q_t$ točno.
\begin{equation}
p=Sigmoid(W^T_1f_t)
\end{equation}


\subsection{Proces ažuriranja}

Proces ažuriranja mijenja vrijednosti matrice $M^v_t$ koja predstavlja trenutno stanje studentovog koncepta znanja \textit{k}.
Ovaj model je drukčiji od DKVMN po tome da se razmatra i trajanje rješavanja. Prema povezanim radovima, trajanje rješavanja zadatka je povezano sa razinom znanja studenta. Pošto je vrijeme rješavanja kontinuirana varijabla, ona se prvo diskretizira po njenoj distribuciji i onda se prikaže kao varijabla $t$ te se koristi kako bi se ažurirala matrica $M$. Ostali procesi su isti kao i u DKVMN, sastoji se od podprocesa dodavanja i brisanja. Vektor brisanja dobije se kao $e=Sigmoid(E^T[s_t,t])$, dok se vektor dodavanja dobije kao $a=Tanh(D^T[s_t,t])$. Nova matrica M računa se kao
\begin{equation}
	M^v_{t+1}(i+1) =M^v_t (i)[1-w(i)e][1+w(i)].
\end{equation}
Parametri ovog modela se račuaju tako da se minimizira "cross-entropy loss" između pretpostavljenog studentovog
dobvivenog i pretpostavljenog rezultata. 
\begin{equation}
	L=-\sum_t ((y_i \log p_t ) + (1-y_t)\log (1-p_t))
\end{equation}






\section{Preporuka zadataka podržanim učenjem}
\subsection{Općenito o podržanom učenju}
Glavni elementi svakog algoritma podržanog učenja su, osim agenta i okoline, strategija, nagrada, vrijednost (predikcija nagrade) i opcionalno model. Strategija predstavlja agentov način ponašanja u određenom trenutku. Ona je preslikavanje iz spoznatog stanja okoliša u akcije koje je potrebno izvršiti u tim stanjima. Agentova je srž, ona sama je dovoljna da odredi ponašanje. \newline
Cilj podržanog učenja definira nagrada koja preslikava opaženo stanje okoliša (ili par stanje-akcija) u jedan broj koji odgovara poželjnosti tog stanja. Cilj agenta je maksimizirati ukupnu nagradu. Nagrada definira što je trenutno dobro, a funkcija vrijednosti definira što je dugoročno dobro. Vrijednost stanja je očekivanje ukupne količine nagrade koja bi se mogla akumulirati u budućnosti počevši od tog stanja. To očekivanje još se naziva i Q funkcija.\newline
Model imitira ponašanje okoline. Za dano stanje i akciju, model može predvidjeti rezultantno iduće stanje i nagradu.\newline
\newline
U ovom radu okolina se izgrađuje simulatorom temeljenim na DKVMN-CA (CA = \textit{concept aware}). Personalizirani agent za preporučivanje zadataka trenira se dubokim učenjem, točnije korištenjem GRU-a (engl. \textit{Gated Recurrent Unit}): poboljšane verzije standarnih RNN. \newline
\newline
Proces odlučivanja modelira se kao Markovljev proces odlučivanja s djelomično vidljivim stanjima (POMDP).
POMDP je generalizacija Markovljevog procesa odlučivanja. Modelira agentov proces odlučivanja; vezu agenta i okoline.
Formalno, POMDP je skup 7 varijabli (stanje, akcija, uvjetna vjerojatnost prijelaza među stanjima, nagrada, opažanja, uvjetne vjerojatnosti opažanja, koeficijent umanjenja nagrade). Agent u svakom stanju bira akciju koja će maksimizirati buduću nagradu umanjenu za faktor umanjenja.\newline
Osnovna je pretpostavka u podržanom učenju da agent vidi stvarno stanje svijeta. No, agentova opažanja u stvarnom svijetu nisu nužno isto što i pravo stanje: uvodi se vjerojatnost stanja $p(o_i|s_i)$ - vjerojatnost da se za opažanje $o_i$ svijet nalazi u $s_i$. Sljedeće stanje u svijetu ovisi o trenutnom stanju i agentovoj akciji. Dva stanja svijeta mogu rezultirati u jednakom opažanju agenata. Za opažanja ne vrijedi Markovljevo svojstvo (sljedeće opažanje stanja ne ovisi isključivo o trenutnom opažanju i akciji). Potrebno je uzeti u obzir putanju agenta. \newline
\newline
\subsection{Primjena na ExRec}
Konkretno, u ExRec-u stanje modela je studentovo konkretno latentno znanje, a akcija je preporuka zadatka. Strategija preporuka direktno funkcionira na sirovim opažanjima studentove povijesti rješavanja zadataka. U trenutku $t$ agent ne može vidjeti studentovo znanje $s_t$. No, opažanje mu je $o_t$ - zadatak i točnost rješavanja uvjetovani pomoću $s_t$, $p(o_t|s_t)$. Agent preporučuje $a_t$ na temelju povijesti studentovih zadataka $h_t=(o_1, a_1, o_2, a_2, ..., o_{t-1},a_{t-1})$. Nakon završetka preporučenog zadatka, latentno znanje prelazi u $s_{t+1}$ pomoću $p(s_{t+1}|s_t, a_t)$.\newline
Agentu je potrebna strategija kako bi izabrao najbolju akciju za određeno stanje. Nagrada akcije $a_t$ je usrednjena suma vjerojatnosti da će student točno riješiti sljedeći zadatak, $q$, nakon što riješi predloženi zadatak u stanju $s_{t+1}$. Usrednjava se brojem zadataka. Taj sljedeći zadatak predviđen je simulatorom. Algoritam preporučuje zadatak $q$, čija je nagrada maksimalna. Opisano je vidljivo jednadžbom ~\ref{eq:nagr_akc}.\newline
\begin{equation}
r_{t}=\frac{1}{K} \sum_{i=1}^{K} P_{t+1}\left(q_{i}\right)\label{eq:nagr_akc}
\end{equation}
\newline
\newline
Krajnji cilj je maksimizirati nagradu određene strategije gdje su trajektorije $(s_1, o_1, a_1, ...)$ preuzete iz distribucije trajektorija induciranih strategijom $\pi$. Nagrada je izražena jednadžbom ~\ref{eq:nagr_strat}. \newline
\begin{equation}
R=\mathbb{E}_{\tau}\left[\sum_{t=1}^{\infty} \gamma^{t-1} r\left(s_{t}, a_{t}\right)\right]\label{eq:nagr_strat}
\end{equation}
\newline
POMDP se rješava pomoću TRPO algoritma (engl. \textit{Trust Region Policy Optimization}). TRPO je algoritam optimizacije robustan na velik raspon zadataka dajući monotono poboljšanje izmjenom malog broja parametara. Algoritam stalno iznova optimizira lokalne aproksimacije očekivane povratne vrijednosti strategije s kaznom KL-divergencije.\newline
\newline
\section{Evaluacija performansi}
%(ovo ce možda trebati preraditi jer ste mijenjali dataset) izbaciti čak?
Originalni ExRec model evaluiran je na IPS datasetu provodenjem 50 eksperimenata podjelom korisnika na skupove za treniranje i testiranje u omjeru 70:30. Kao mjera performansi odabrana je AUC krivulja. Procjenjena je i efikasnost korištenjem dodatnih značajki modela, poput težine zadataka, faze rješavanja i trajanja rješavanja čija upotreba dokazano poboljšava performanse modela. 
\subsection{Preporuke zadataka}
\subsubsection{Evaluacija porasta znanja studenta}
Algoritmi za treniranje strategije preporuke zadataka pomoću podržanog učenja većinom su heuristički; zadatke ocijenjene kao prelagane ili preteške potrebno je izbjegavati. No, optimalnost takvih algoritama nije zadovoljena jer se u obzir uzima samo kratkoročna nagrada. U ExRec-u se u obzir uzima dugoročna nagrada. \newline
%izbaciti ako nećemo koristiti do kraja?
Kao strategije algoritma podržanog učenja u originalnom ExRec-u korištena su dva algoritma, Expectimax i RL. Kod Expectimaxa prvo se računa iznos predviđenog znanja u slučaju da se preporuči određen zadatak. Sustav kao preporuku izabire zadatak kojim bi maksimizirao predviđeno znanje u tom trenutku. RL razmatra dugoročnu nagradu akcije maksimizacijom Q funkcije.\newline
%(isto izbaciti/izmijeniti ovisno kako cemo mi raditi)
Za usporedbu algoritama, autori rada izvlače 15 studenata iz dataseta. Za oba algoritma inicijaliziraju se simulatori pomoću slijeda prethodno riješenih zadataka svakog studenta. Preporuča se 50 zadataka. Sprema se prosjek predviđenog znanja svih studenata u svakom koraku preporuke. %ubaciti sliku


\subsubsection{Evaluacija preporuka} %izaciti ako nećemo koristiti?
Za proučavanje RL strategije autori su osmislili dodatni eksperiment kojim se uzme student i njegovih prijašnje riješenih 5 zadataka te pokrene simulator. Preporuči mu se novih 5 zadataka pomocu RL. Vizualno se prikaže tih 10 zadataka u obliku (broj zadatka, povezani koncept, točnost). Promatra se povezanost točnosti zadataka i odgovarajućih koncepata. Ako ne postoji znanje o nekom konceptu, grafički prikaz je crne boje. Kako znanje raste, boja je svjetlija.\newline
Kad student netočno riješi zadatak, algoritam mu preporuči povezani koncept. Ako padne i preporučeni zadatak, sustav mu nudi ponovno rješavanje.\newline
Nakon povećanja znanja o nekom konceptu, prebacuje se na neki drugi koncept. Ako je zadatak okarakteriziran kao lagan, znanje se ne povećava pretjerano iako je točno riješen.\newline
Ponovno se preporučuje početni zadatak i stvar se ponavlja dok se ne riješi točno.


\subsection{Rezultati i problemi}
 	\subsubsection{26.8.2020.}
 		Prilagođena je skripta new\_kt.py kako bi davala parametre za biologiju (smanjen je seq\_len), napravljene su skripte koje stvaraju potrebne .pkl fileove bez da su putevi do datoteka hardkodirani. Također, napravljena je skripta koja dataset dijeli na dio za treniranje i dio za validaciju u formatu koji treba za pokretanje new\_kt.py. Nakon što su broj koncepata i jedinstvenih pitanja postavljeni da odgovaraju datasetu biologije new\_rs.py se može normalno pokrenuti.
\newline
Za razliku od Assistmentsa vrijednosti porasta znanja u svakom koraku su oko 0.507 te slabo osciliraju.
 		\begin{figure}[!htb]
 			\centering
 			\includegraphics[scale=0.8]{exrec_bio1.png}
 			\caption{}
 			\label{}
 		\end{figure}
 	
 	\begin{figure}[!htb]
 		\centering
 		\includegraphics[scale=0.8]{exrec_bio2.png}
 		\caption{}
 		\label{}
 	\end{figure}
 
 	I dalje nije jasno koje vrijednosti hiperparametara poput seq\_len i veličina memorije bi se trebali koristiti te koje je pravo značenje "porasta/vrijednosti" znanja u svakom trenutku. Za iste parametre i dataset new\_rs.py uvijek daje drugačiji put preporuke što nije poželjno. Potrebno je i omogućiti izvedbu RS-a po chunkovima Assistmentsa koji nisu hardkodirani. Iz nekog razloga kada se koristi biologija trebaju se zakomentirati sve "isnan" funkcije, dok su iste potrebne kod Assitmentsa.
	
	\subsubsection{01.09.2020.}
		Napravljena je skripta koja automatizira pokretanje ExRec-a za jednostavne datasetove poput "Biologije". Trenutno je potrebno samo znati ime dataseta i put do direktorija u kojem je dataset. Ostalo je shvatiti kako prilagoditi model kt-a, generiranje train i validation dijelova dataseta kako se ne bi stvarali privremeni .csv fileovi. Također treba još proučiti shvatiti zašto kod učitavanja assistmentsa treba raditi dodatne provjere Nan-ova, dtype-a te imena stupaca, dok u isto vrijeme te iste provjere rade grešku kod dataseta "Biologija".
		Trenutne ideje za potencijalne preporuke zadataka:
		\begin{itemize}
			\item svi zadaci- nedostatak je sto je jako sporo
			\item svi ostali zadaci iz koncepata kojih se korisnik dotakao- brže, ali i dalje prepsoro
			\item potencijalno korištenje clusteringa/BKT-a za preporuku manjeg skupa zadataka
		\end{itemize}

\subsubsection{04.09.2020. - eksperimentiranje s parametrima kt dijela}
Provedeni su eksperimenti parametara, argumenata kt dijela - broja epoha, stope učenja, brzine smanjivanja stope učenja, momentuma koji ubrzava gradijentni spust, omjera rezanja tenzora kako gradijent ne bi eksplodirao te utjecaj \textit{batch sizea} dataseta.\newline
Najznačaniji zaključak izveden je za broj epoha - povećanjem rastu iznosi izlaznih vjerojatnosti. Eksperimentirano je s vrijednostima 1, 5, 100 i 1000. Najznačaniji rezultati dobivaju se za 1000, no dugotrajnost treniranja je jako izražena. Stoga je većina sljedećih eksperimenata provođena sa 100 epoha. Ostali parametri većinom zahtijevaju neznatne izmjene u odnosu na originalno postavljene.\newline
Isprobane su vrijednosti [0.005, 0.05, 0.5]. Stopa učenja ne smije imati prevelik iznos jer rezultat pokazuje sumnjivo previsoku izlaznu vjerojanost, blizu 1, što ukazuje na prenaučenost sustava. Potrebno je odgovarajuće intervalno smanjivanje stope učenja, koje ne smije biti preveliko jer prebrzo konvergira, niti premalo jer je vrijeme izvođenja tada presporo iako su rezultati intuitiviji. Ako je potrebno birati, bolje manja stopa učenja i sporiji interval promjene nego veća stopa i veće promjene.\newline
Istražen je i momentum parametar. Sa samim stohastičnim gradijentnim spustom, SGD, ne računamo točnu derivaciju funkcije gubitka, već estimiramo na manjim \textit{batchevima}. Iz tog razloga, ne ide se uvijek u optimalnom smjeru jer takve derivacije zašumljene. Kod SGD-a su isto tako problem tjesnaci strmiji u jednoj dimenziji (česti su blizu lokalnih minimuma). SGD ih teško uočava. Dodavanje Momentum algoritma ubrazava njihove prelaske. Također, derivacije su glađe, manje zašumljene. Provedeni su eksperimenti s vrijednostima između 0.5 i 0.95. Što je parametar Momentuma manji, slijed podataka je više skokovit. Algoritam usrednjava na manjem broju podataka pa smo bliže zašumljenim podacima. Utvrđeno je da je najbolji momentum 0.9, što je vrijednost koja se često uzima u strojnom učenju.\newline
Isprobane su i različite veličine maxgradnorm parametra. Naime, kako gradijenti ne bi eksplodirali tijekom treniranja, koriste se tehnike \textit{gradient clippinga}. Eksplodiranje gradijenta dogodi se kad je gradijent prevelik, što dovodi do akumuliranja pogrešnih gradijenata. Kao rezultat dobije se nestabilna mreža. Iz tog razloga, u TensorFlow-u koristi se naredba \textit{tf.clip\_by\_global\_norm()} - reže tenzore u treniranju u omjeru sume njihovih normi, a omjer definira upravo maxgradnorm argument. Povećanjem i smanjenjem vrijednosti [5, 100], otkriveno je da se rezultati ne mijenjaju značajno te je stoga najbolje ostaviti originalno (50). \newline
Što se tiče \textit{batch sizea}, za "Biologiju" se pokazao boljim manji \textit{batch} [5, 15, 32 -> 5], prilagođeniji činjenici da je sam dataset malen. 

\subsubsection{07.09.2020. - eksperimentiranje s parametrima kt i rs dijela}
Variranjem kt parametara \textit{memory\_key\_state\_dim, memory\_value\_state\_dim} te \textit{final\_fc\_dim} zaključeno je da njihova veličina ne utječe na izlazne vrijednosti, već samo doprinosi brzini treniranja (ako manja veličina, brže se trenira). Već je prije zaključeno i opet potvrđeno kako paramteri \textit{memory\_size, n\_questions} i \textit{seq\_len} moraju točno odgovarati vrijednostima dataseta: broju koncepata, broju pitanja te duljini niza treniranja.  \newline
U rs dijelu, dosadašnji eksperimenti većinom su izvođeni s jednom epohom treniranja RLTutora. Variranjem epoha na 10 i 100, nije došlo do značajnijih poboljšanja (ni povećanja ni stabilnosti) u rezultatu izlaza. \newline
S druge strane, ako se parametar \textit{hidden\_dim} podesi na "Biologiji" prikaldnijih 5, s obzirom na postojanje 5 koncepata u datasetu (u odnosu na originalno postavljenih 32) zajedno s \textit{batch\_sizeom}, dobije se porast izlaza. Ipak, prilikom različitih pokretanja, izlazi imaju trend pada.\newline
Ako se \textit{batch\_size raw\_policyja LoggedTRPO} smanji s originalnih 4000 na prihvatljivijih 32, uz parametar \textit{batch\_size} kt-a i {hidden\_dim} rs-a postavljen na 5, dobije se veća stabilnost. Ako se pritom broj epoha treniranja kt dijela poveća s 1 na 100, dobiju se znatno veće vrijednosti izlaza u odnosu na prethodna razmatranja (npr. \newline outList [0.7687031   0.8171221  0.83575428] \newline umjesto dosadašnjih \newline outList [0.5994091  0.59953147 0.59946656].\newline
U Categorical GRU strategiji podržanog učenja (originalno postavljenoj), napravljeni su eksperimenti s postavljanjem različitih aktivacijskih funkcija koje u treniranje uvode nelinearnosti. Originalno je postavljen tanh. Postav sigmoide većinom povećava izlazne vrijednosti i njihov razmak, ali uvodi trend pada. Softmax ne pokazuje zadovoljavajuće rezultate; smanjuje izlazne vrijednosti i njihov razmak. Rectify (linear rectifier) aktivacijska funkcija preporučuje stalno isti zadatak u putanji, ali ne uspijeva se doći do točnog rješenja. Leaky\_rectify se ponaša previše nepredvidljivo; vrijednosti skaču i padaju. Selu i linear aktivacijske funkcije ne pokazuju pravilnosti pri izvođenju. Elu (engl. \textit{exponential linear unit}) pokazuje dosta dobra svojstva. Smanjuje \textit{bias} (pristranost pomaka) u odnosu na rectify  odmicanjem srednje aktivacije prema 0. Brže konvergira nuli i proizvodi točniji rezultat (jer nema negativan gradijent za negativne vrijednosti). Primjeri rješenja za "Biologiju":\newline
Preporuceni put: [(1, 1), (0, 1), (2, 0)]\newline
outList [0.68188673 0.68203878 0.77824879]\newline
Preporuceni put: [(2, 1), (1, 1), (3, 1)]\newline
outList [0.75084096 0.79288167 0.797158 ].\newline
Eksperimentirano je i s vrijednostima \textit{discounta} (između 0.5 i 0.99) u \textit{Logged TRPO} i \textit{n\_steps} (2, 5, 10, 50) u definiciji okoline. Zaključeno je kako nije moguće definirati pravilnosti jer su rezultati previše skokoviti, previše osciliraju.\newline
Provođene su i provjere ponašanja različitih strategija. Strategija je smatrana najvažnijim aspektom podržanog učenja agenta i zahtijeva određeni dugotrajniji \textit{tuning}. U ExRec-u je originalno korištena strategija Categorical GRU koja sadrži GRU koji predviđa na temelju kategoričke distribucije. Kategoričke strategije su namijenjene diskretnim akcijskim prostorima. Očekuju da akcijske vrijednosti predstavljaju vjerojatnosnu distribuciju prema akciji. Iz takve distribucije svaka je akcija uzorkovana. \newline
Uz spomenutu strategiju, najviše obećavajućom pokazala se Categorical MLP strategija koja sadrži MLP (engl. \textit{multi-layer perceptron} koji predviđa na temelju kategoričke distribucije. Uz zanemarivanje originalnih vrijednosti i postav \textit{hidden\_size = (5, 5)} dolazi do stalnog rasta izlaza, ali preporuka često uključuje isti zadatak koji korisnik nikako da točno riješi.\newline
Preporuceni put: [(0, 1), (3, 0), (3, 0), (3, 0)]\newline
outList [0.59386724 0.5939555  0.5939993  0.5940211 ]\newline
Ako se koristi Categorical MLP uz \textit{hidden\_size = (5, 5)} i elu aktivacijsku funkciju, dolazi do veće stabilnosti uz trend rasta izlaznih vrijednosti. Naime, prilikom različtih pokretanja, sustav preporučuje često preporučuje isti put za isti prijeđeni \textit{trace}, što dosad nije bio slučaj.\newline
Preporuceni put: [(3, 1), (2, 1), (1, 1)]\newline
outList [0.59849751 0.59855151 0.59854913]\newline
Preporuceni put: [(2, 1), (3, 1), (1, 1)]\newline
outList [0.59207004 0.59207171 0.5922001 ]\newline

\subsubsection{08.09.2020. - optimizatori}
TRPO algoritam, već spomenut prilikom objašnjavanja pozadine funkcioniranja podržanog učenja u ExRec-u iterativni je pristup optimizaciji strategije sa zagarantiranim monotonim napretkom.\newline
Koristi \textit{Natural Policy Gradient} prisup koji analitički rješava funkciju cilja. Problem je što se u izračunu koristi inverz Hesseove matrice drugih derivacija log vjerojatnosti strategije (Fisherove informacijske matrice), a on je jako kompliciran ako je strategija paramtrizirana brojnim parametrima. Također, inverz je često nestabilan. Kako se on ne bi morao eksplicitno tražiti, koristi se aproksimacija izraza u krnjoj verziji pristupa, \textit{Truncated Natural Policy Gradient}. Za optimizaciju tog izračuna koristi se metoda konjugatnih gradijenata. \newline
Koncept je vrlo sličan gradijentnom uzdizanju, ali ga je moguće izvesti u manje iteracija. U gradijentnom usponu, uvijek se prati najstrmiji gradijent. Putanja od konačne do početne točke može biti neoptimalna, "cik-cak". Takvu neefikasnost moguće je izbjeći konjugatnim gradijentom (ako je funkcija cilja kvadratna). Ako model ima N parametara, moguće je naći optimalnu točku u najviše N uzdizanja. U prvom potezu slijedi se smjer najdubljeg gradijenta i definira se optimalna točka u tom smjeru. Sljedeći smjer mora biti ortogonalan prema nekoj transformacijskoj matrici A (konjugatan) svim prethodnim smjerovima. Takav izračun mnogo je manje zahtjevan od izračuna inverza Hesseove matrice.\newline
Ideja je bila provjeriti kako TRPO funkcionira s ostalim optimizacijama, tj. različitim pristupima aproksimaciji. \newline
Ako se koristi optimizator prvog reda, za originalne parametre, rs dio produljuje vrijeme izvođenja (s par sekundi na preko 2 min). Vrijednosti vrlo malo osciliraju oko 0.50. 
Ako se, pak, parametri izmijene prema onima koji su generirali dobre rezultate prethodnih dana (u kt broj epoha 100, batch\_size = 5, u rs hidden\_dim = 5, batch\_size = 32, aktivacijska funkcija = elu), dobiju se zadovoljavajući rezultati.\newline
Preporuceni put: [(0, 0), (2, 0), (0, 1)]\newline
outList [0.57768267 0.68047982 0.70207518]\newline
Preporuceni put: [(0, 1), (3, 1)]\newline
outList [0.72096705 0.72513175]\newline
No, dobri rezultati nisu uvijek slučaj. Literatura spominje da optimizatori prvog reda nisu točni u područjima zakrivljenosti. Naime, s derivacijama prvog reda često se površina kojom putuje prilikom traženja gradijenta aproksimira glatkom. Ako je vrlo zakrivljena, potezi su jako loši.\newline
Preporuceni put: [(3, 1), (2, 0), (2, 0), (2, 0)]\newline
outList [0.3331522  0.29257703 0.2808274  0.27639025] \newline
Preporuceni put: [(2, 0), (3, 1), (2, 0), (0, 0)]\newline
outList [0.52635247 0.487258   0.38557106 0.35406634]\newline
Za kontroliranje stope učenja originalno je korištena Adam metoda. Eksperimentima s Adadeltom te SGD + Momentum metodama nije došlo do većih odstupanja u rezultatima. \newline
Osim gradijenta prvog reda i konjugatnog gradijenta, proučen je i isproban LBFGS optimizator. Metoda je to optimizacije koja koristi derivacije drugog reda. Kvazi-Newtonova je metoda; metoda aproksimacije Hesseove matrice.\newline
Jedna od najpopularnijih metoda aproksimacije Hesseove matrice je BFGS metoda temeljena na kompletnoj povijesti gradijenata. 
LBFGS metoda (engl. \textit{limited memory BFGS}) je temeljena na zadnjih m gradijenata. Popularna je jer je potrebno zadržati samo tih zadnjih m gradijenata (obično 10 do 20) što doprinosi smanjenju skladišnog prostora. Za razliku od potpunog BFGS-a, LBFGS nikad eksplicitno ne formira ili sprema procjenu Hesseove matrice.\newline
Iako u teoriji obećavajuće, u praksi se većinom primjećuje preporuka samo jednog zadatka ili je pak vidljiv trend pada koji nije u skladu s našim zahtjevima.
Preporuceni put: [(0, 1), (2, 1)]\newline
outList [0.60839105 0.60837907]\newline
Preporuceni put: [(1, 0), (1, 1)]\newline
outList [0.60479879 0.60473955]\newline
Preporuceni put: [(1, 1)]\newline
outList [0.59713161]\newline

\section{Upute za pokretanje}

Za pokretanje ove verzije ExReca potrebno je imati dataset koji ima slijedeće stupce:

\begin{itemize}
	 \item user\_id - identifikacijska oznaka korisnika
	 \item problem\_id -identifikacijska oznaka zadatka
	 \item skill\_id -identifikacijska oznaka koncepta
	 \item correct - točnost rješenog zadataka (0 ili 1)
\end{itemize}

U skripti "sve\_u\_jednom.py" su napisane sve potrebne naredbe za pokretanje ExReca nad nekim datasetom.
Prvo je potrebno dati putanju do .csv filea dataseta, zatim se u funkciji"get\_chunks(path)" skripte chunk\_analysis
dataset dijeli na manje dijelove radi lakšeg pokretanja, ta funkcija ima i opcionalan argument chunk\_size s
kojim se kontrolira veličina chunka. Ako je chunk\_size veći nego dataset, uzima se cijeli skup podataka. Funkcija
vraća objekt tipa pandas dataframe. Dalje se željeni dataframe šalje u konstruktor klase ChunkInfo skripte chunk\_analysis
gdje se obrađuju određena svojstva dataframea i pakiraju se varijable koji će se koristiti za pravilno funkcioniranje ExReca.

Prvi dio ExReca je Knowledge Tracing dio i za njegovo funkcioniranje potrebno je podijeliti dataset na train i validation dio
to rade create\_from\_dataframe i create\_from\_csv funkcije skripte train\_and\_validate\_creator. Ta funkcija ima opciju
čitanja dataseta iz csv-a ili dataframea. Također ima opciju želi pisati nove datasetove kao .csv fileove ili da ih vraća
kao dataframe varijable. Omjer train:valid je po defaultu stavljen na 7:3, ali postoji i argument funkcije kojim se i to može mijenjati.
KT dio exreca računa parametre koji su potrebni za funkcioniranje Recommendation System dijela i vraća ih u varijablu params.
Params se zajedno sa argumentima dobivenih iz chunk\_analysisa šalju u RS dio gdje se generira pretpostavljeni "put" odabira i
točnosti korisnikovih zadataka te ujedno daje i procjenjenu razinu znanja u svakom trenutku.



\section{Poveznice}
\url{http://incompleteideas.net/book/first/ebook/node9.html}\newline
\url{https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be}\newline
\url{https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process}\newline
\url{https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9}\newline
\url{https://arxiv.org/abs/1502.05477}\newline
\url{https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d}\newline
\url{https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/MomentumOptimizer}\newline
\url{https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping}\newline
\url{https://stackoverflow.com/questions/49987839/how-to-handle-none-in-tf-clip-by-global-norm}\newline
\url{https://lasagne.readthedocs.io/en/latest/modules/nonlinearities.html}\newline
\url{https://garage.readthedocs.io/en/v2020.06.0/_apidoc/garage.tf.policies.categorical_gru_policy.html}\newline
\url{https://nervanasystems.github.io/coach/components/exploration_policies/index.html}\newline
\url{https://stable-baselines.readthedocs.io/en/master/modules/trpo.html}\newline
\url{https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a}\newline
\url{https://stats.stackexchange.com/questions/284712/how-does-the-l-bfgs-work/285106}


